{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet Basics - Linear Regression using MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Scala kernel\n",
    "Add mxnet scala jar which is created as a part of MXNet Scala package installation in classpath as follows:\n",
    "\n",
    "**Note**: Process to add this jar in your scala kernel classpath can differ according to the scala kernel you are using.\n",
    "\n",
    "We have used [jupyter-scala kernel](https://github.com/alexarchambault/jupyter-scala) for creating this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "classpath.addPath(<path_to_jar>)\n",
    "\n",
    "e.g\n",
    "classpath.addPath(\"mxnet-full_2.11-osx-x86_64-cpu-0.1.2-SNAPSHOT.jar\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.io.{NDArrayIter}\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.module.{FitParams, Module}\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.optimizer.SGD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.Callback.Speedometer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ml.dmlc.mxnet._\n",
    "import ml.dmlc.mxnet.io.{NDArrayIter}\n",
    "import ml.dmlc.mxnet.module.{FitParams, Module}\n",
    "import ml.dmlc.mxnet.optimizer.SGD\n",
    "import ml.dmlc.mxnet.Callback.Speedometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "MXNet uses data in the form of **Data Iterators**. The code below illustrates how to encode a dataset into an iterator that MXNet can use. The data used in the example is made up of 2d data points with corresponding integer labels. The function we are trying to learn is:\n",
    "\n",
    " y = x<sub>1</sub>  +  2x<sub>2</sub> ,\n",
    " \n",
    " where (x<sub>1</sub>,x<sub>2</sub>) is one training data point and y is the corresponding label. \n",
    "\n",
    "e.g. First label 5 is generated as follows:\n",
    "\n",
    "5 = 1 + 2*2 (where x1 = 1, x2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (MXNetJVM).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mVector\u001b[0m(ml.dmlc.mxnet.NDArray@57e47ffa)\n",
       "\u001b[36mtrainLabel\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mVector\u001b[0m(ml.dmlc.mxnet.NDArray@a040dbc2)\n",
       "\u001b[36mbatchSize\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m1\u001b[0m\n",
       "\u001b[36mevalData\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mVector\u001b[0m(ml.dmlc.mxnet.NDArray@ed4b006d)\n",
       "\u001b[36mevalLabel\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mVector\u001b[0m(ml.dmlc.mxnet.NDArray@f8bc2cd5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Training data\n",
    "val trainData = IndexedSeq(NDArray.array(Array(1, 2, 3, 4, 5, 6, 3, 2, 7, 1, 6, 9), shape = Shape(6, 1, 2)))\n",
    "val trainLabel = IndexedSeq(NDArray.array(Array(5, 11, 17, 7, 9, 24), shape = Shape(6)))\n",
    "val batchSize = 1\n",
    "\n",
    "//Evaluation Data\n",
    "val evalData = IndexedSeq(NDArray.array(Array(7, 2, 6, 10, 12, 2), shape = Shape(3, 1, 2)))\n",
    "val evalLabel = IndexedSeq(NDArray.array(Array(11, 26, 16), shape = Shape(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data ready, we need to put it into an iterator and specify parameters such as the 'batch_size', and 'shuffle' which will determine the size of data the iterator feeds during each pass, and whether or not the data will be shuffled respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainIter\u001b[0m: \u001b[32mNDArrayIter\u001b[0m = non-empty iterator\n",
       "\u001b[36mevalIter\u001b[0m: \u001b[32mNDArrayIter\u001b[0m = non-empty iterator"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val trainIter = new NDArrayIter(trainData, trainLabel, batchSize, false, \"pad\")\n",
    "val evalIter = new NDArrayIter(evalData, evalLabel, batchSize, false, \"pad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have made use of NDArrayIter, which is used to iterate over numpy arrays. In general, there are many different types of iterators in MXNet based on the type of data you will be using. Their complete documentation can be found at [Scala API](http://mxnet.io/api/scala/docs/index.html#package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet Classes\n",
    "\n",
    "1. [Model Class](http://mxnet.io/api/scala/model.html): The model class in MXNet is used to define the overall entity of the model. It contains the variable we want to minimize, the training data and labels, and some additional parameters such as the learning rate and optimization algorithm are defined at the model level.\n",
    "\n",
    "2. [Module Class](http://mxnet.io/api/scala/module.html): The module class provides an intermediate and high-level interface for performing computation with neural networks in MXNet.\n",
    "\n",
    "3. [Symbols](http://mxnet.io/api/scala/symbol.html): The actual MXNet network is defined using symbols. MXNet has different types of symbols, including data placeholders, neural network layers, and loss function symbols based on our requirement.\n",
    "\n",
    "4. [IO](http://mxnet.io/api/scala/io.html): The IO class as we already saw works on the data, and carries out operations like breaking the data into batches and shuffling it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXNet uses **Symbols** for defining a model. [Symbols](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.Symbol) are the building blocks of the model and compose various components of the model. Some of the parts symbols are used to define are:\n",
    "1. Variables: A variable is a placeholder for future data. This symbol is used to define a spot which will be filled with training data/labels in the future when we are trying to train the model.\n",
    "2. Neural Network Layers: The layers of a network or any other type of model are also defined by Symbols. Such a *symbol* takes one of the previous symbols as its input, does some transformation on them, and creates an output. One such example is the \"Fully Connected\" symbol which specifies a fully connected layer of a network. \n",
    "3. Output Symbols: Output symbols are MXNet's way of defining a loss. They are suffixed with the work \"Output\" (eg. the SoftmaxOutput layer\" . You can also create your [own loss](https://github.com/dmlc/mxnet/blob/5b6a0eeee174f28ff0272d17748513ecd52a9ebe/docs/tutorials/r/CustomLossFunction.md#how-to-use-your-own-loss-function). Some examples of existing losses are: LinearRegressionOutput, which computes the l2-loss between it's input symbol and the actual labels provided to it, SoftmaxOutput, which computs the categorical cross-entropy. \n",
    "\n",
    "The ones described above, and other symbols are chained one after the other, servng as input to one another to create the network topology. More information about the different types of symbols can be found [here](http://mxnet.io/api/scala/symbol.html)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@78525c12\n",
       "\u001b[36mlabel\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@4e85112e\n",
       "\u001b[36mfc1\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@3c61948a\n",
       "\u001b[36msoftmax\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@5ecd7ee4\n",
       "\u001b[36mres4_4\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArrayBuffer\u001b[0m(\u001b[32m\"data\"\u001b[0m, \u001b[32m\"fc1_weight\"\u001b[0m, \u001b[32m\"fc1_bias\"\u001b[0m, \u001b[32m\"label\"\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val data = Symbol.Variable(\"data\")\n",
    "val label = Symbol.Variable(\"label\")\n",
    "val fc1  = Symbol.FullyConnected(\"fc1\")()(Map(\"data\" -> data, \"num_hidden\" -> 1))\n",
    "val softmax = Symbol.LinearRegressionOutput()()(Map(\"data\" -> fc1, \"label\" -> label))\n",
    "softmax.listArguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above network uses the following layers:\n",
    "\n",
    "1. FullyConnected: The fully connected symbol represents a fully connected layer of a neural network (without any activation being applied), which in essence, is just a linear regression on the input attributes. It takes the following parameters:\n",
    "            a. data: Input to the layer (specify the symbol whose output should be fed here)\n",
    "            b. num_hidden: Number of hidden dimension which specifies the size of the output of the layer\n",
    "    \n",
    "    \n",
    "2. Linear Regression Output: Output layers in MXNet aim at implementing a loss. In our example, the Linear Regression Output layer is used which specifies that an l2 loss needs to be applied against it's input and the actual labels provided to this layer. The parameters to this layer are:\n",
    "            a. data: Input to this layer (specify the symbol whose output should be fed here)\n",
    "            b. Label: The training label against whom we will compare the input to the layer for calculation of l2 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note - *Naming Convention*: the label variable's name should be the same as the label_name parameter passed to your training data iterator. The default value of this is \"softmax_label\", but we have updated it to label in this tutorial as you can see in val label = Symbol.Variable(\"label\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the network is stored into a *Model*, where you define the symbol who's value is to be minimised (in our case, softmax\"), the learning rate to be used while optimization and the number of epochs we want to train our model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the network we have created in order to visualize it and save it by specifying \"path\" in `dot.render()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdot\u001b[0m: \u001b[32mVisualization\u001b[0m.\u001b[32mDot\u001b[0m = ml.dmlc.mxnet.Visualization$Dot@72a23b8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dot = Visualization.plotNetwork(symbol=softmax, nodeAttrs = Map(\"shape\" -> \"oval\", \"fixedsize\" -> \"false\") )\n",
    "dot.render(engine = \"dot\", fileName = \"linearRegression\", path = \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the model structure, the next step is to train the parameters of the model to fit the training data. This is done by using the **fit()** function of the **Module** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmod\u001b[0m: \u001b[32mModule\u001b[0m = ml.dmlc.mxnet.module.Module@76f4d37f"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val mod = new Module(softmax, labelNames = IndexedSeq(\"label\"))\n",
    "\n",
    "mod.fit(trainData = trainIter, evalData = scala.Option(evalIter), numEpoch = 1000, fitParams = new FitParams()\n",
    "    .setOptimizer(new SGD(learningRate = 0.01f, momentum = 0.9f, wd = 0.0001f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use [FeedForward network](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.FeedForward) and use [Model API](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.Model) of MXNet to build the model instead of Module. This can be done as follows:\n",
    "\n",
    "```scala\n",
    "    val model = new FeedForward(symbol = softmax, ctx = Context.cpu(0), numEpoch = 1000, optimizer = new SGD(learningRate = 0.01f, momentum = 0.9f, wd = 0.0001f))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained model: (Testing and Inference) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a trained model, we can do multiple things on it. We can use it for inference, we can evaluate the trained model on test data. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mprobArrays\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mArrayBuffer\u001b[0m(\n",
       "  ml.dmlc.mxnet.NDArray@85887f13,\n",
       "  ml.dmlc.mxnet.NDArray@c68f7cef,\n",
       "  ml.dmlc.mxnet.NDArray@d30a2eee\n",
       ")\n",
       "\u001b[36mprob1\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m11.000008F\u001b[0m)\n",
       "\u001b[36mprob2\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m25.999908F\u001b[0m)\n",
       "\u001b[36mprob3\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m15.999969F\u001b[0m)\n",
       "\u001b[36mname\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"mse\"\u001b[0m\n",
       "\u001b[36mvalue\u001b[0m: \u001b[32mFloat\u001b[0m = \u001b[32m3.1435168E-9F\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val probArrays  = mod.predict(evalIter)\n",
    "\n",
    "val prob1 = probArrays(0).toArray\n",
    "val prob2 = probArrays(1).toArray\n",
    "val prob3 = probArrays(2).toArray\n",
    "\n",
    "val (name, value) = mod.score(evalIter, new MSE()).get\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also evaluate our model for some metric. In this example, we are evaulating our model's mean squared error on the evaluation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to add some noise to the evaluation data and see how the MSE changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mevalData\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mVector\u001b[0m(ml.dmlc.mxnet.NDArray@d5cb35ca)\n",
       "\u001b[36mevalLabel\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mNDArray\u001b[0m] = \u001b[33mVector\u001b[0m(ml.dmlc.mxnet.NDArray@1319931b)\n",
       "\u001b[36mevalIter\u001b[0m: \u001b[32mNDArrayIter\u001b[0m = non-empty iterator"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Evaluation Data\n",
    "val evalData = IndexedSeq(NDArray.array(Array(7, 2, 6, 10, 12, 2), shape = Shape(3, 1, 2)))\n",
    "val evalLabel = IndexedSeq(NDArray.array(Array(11.1f, 26.1f, 16.1f), shape = Shape(3))) //#Adding 0.1 to each of the values \n",
    "\n",
    "val evalIter = new NDArrayIter(evalData, evalLabel, batchSize, false, \"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mname\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"mse\"\u001b[0m\n",
       "\u001b[36mvalue\u001b[0m: \u001b[32mFloat\u001b[0m = \u001b[32m0.010007773F\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val (name, value) = mod.score(evalIter, new MSE()).get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can create your own metrics and use it to evaluate your model. More information on metrics [here](http://mxnet-test.readthedocs.io/en/latest/api/metric.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
