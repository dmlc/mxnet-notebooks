{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "In a recommendation system, there is a group of users and a set of items. Given that each users have rated some items in the system, we would like to predict how the users would rate the items that they have not yet rated, such that we can make recommendations to the users.\n",
    "\n",
    "Matrix factorization is one of the mainly used algorithm in recommendation systems. It can be used to discover latent features underlying the interactions between two different kinds of entities.\n",
    "\n",
    "Assume we assign a k-dimensional vector to each user and a k-dimensional vector to each item such that the dot product of these two vectors gives the user's rating of that item. We can learn the user and item vectors directly, which is essentially performing SVD on the user-item matrix. We can also try to learn the latent features using multi-layer neural networks.\n",
    "\n",
    "In this tutorial, we will work though the steps to implement these ideas in MXNet.\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "We use the [MovieLens](https://grouplens.org/datasets/movielens/) data here, but it can apply to other datasets as well. Each row of this dataset contains a tuple of user id, movie id, rating, and time stamp, we will only use the first three items. We first define the a batch which contains n tuples. It also provides name and shape information to MXNet about the data and label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Scala kernel\n",
    "Add mxnet scala jar which is created as a part of MXNet Scala package installation in classpath as follows:\n",
    "\n",
    "**Note**: Process to add this jar in your scala kernel classpath can differ according to the scala kernel you are using.\n",
    "\n",
    "We have used [jupyter-scala kernel](https://github.com/alexarchambault/jupyter-scala) for creating this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "classpath.addPath(<path_to_jar>)\n",
    "\n",
    "e.g\n",
    "classpath.addPath(\"mxnet-full_2.11-osx-x86_64-cpu-0.1.2-SNAPSHOT.jar\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.util.Random\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.immutable.ListMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.ArrayBuffer\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.optimizer.SGD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.Callback.Speedometer\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.{DataBatch, DataIter, NDArray, Shape}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ml.dmlc.mxnet._\n",
    "import scala.util.Random\n",
    "import scala.io.Source\n",
    "import scala.collection.immutable.ListMap\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.collection.mutable\n",
    "import ml.dmlc.mxnet.optimizer.SGD\n",
    "import ml.dmlc.mxnet.Callback.Speedometer\n",
    "import ml.dmlc.mxnet.{DataBatch, DataIter, NDArray, Shape}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a data iterator, which returns a batch of tuples each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mBDataIter\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BDataIter(filename: String, batch_size: Int) extends DataIter {\n",
    "\n",
    "  val data = ArrayBuffer[(Float, Float, Float)]()\n",
    "  for (line <- Source.fromFile(filename).getLines()) {\n",
    "    val arr = line.split(\"\\t\").map(_.trim)\n",
    "    if(arr.length == 4){\n",
    "      data += ((arr(0).toFloat, arr(1).toFloat, arr(2).toFloat))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val _provideData = ListMap(\"user\" -> Shape(batch_size), \"item\" -> Shape(batch_size))\n",
    "  val _provideLabel = ListMap(\"score\" -> Shape(batch_size))\n",
    "\n",
    "  private var k = 0\n",
    "\n",
    "  override def next(): DataBatch = {\n",
    "    if (!hasNext) throw new NoSuchElementException\n",
    "    val users = ArrayBuffer[Float]()\n",
    "    val items = ArrayBuffer[Float]()\n",
    "    val scores = ArrayBuffer[Float]()\n",
    "    for (i <- 0 to batch_size-1){\n",
    "      val j = k * batch_size + i\n",
    "      val (user, item, score) = data(j)\n",
    "      users += user\n",
    "      items += item\n",
    "      scores += score\n",
    "    }\n",
    "    k +=1\n",
    "    val data_all = Array(NDArray.array(users.toArray, shape = Shape(batch_size)),\n",
    "      NDArray.array(items.toArray, shape = Shape(batch_size)))\n",
    "    val label_all  = Array(NDArray.array(scores.toArray, shape = Shape(batch_size)))\n",
    "\n",
    "    val data_names = Array(\"user\", \"item\")\n",
    "    val label_names = Array(\"score\")\n",
    "\n",
    "    new DataBatch(data=data_all,label=label_all, index=getIndex(), pad=getPad(), providedData=_provideData, providedLabel=_provideLabel)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * reset the iterator\n",
    "    */\n",
    "  override def reset(): Unit = {\n",
    "    scala.util.Random.shuffle(data)\n",
    "    k = 0\n",
    "  }\n",
    "\n",
    "  override def hasNext: Boolean = {\n",
    "    k < (data.length / batch_size)\n",
    "  }\n",
    "\n",
    "  override def batchSize: Int = batch_size\n",
    "\n",
    "  override def getData(): IndexedSeq[NDArray] = IndexedSeq()\n",
    "\n",
    "  override def getIndex(): IndexedSeq[Long] = IndexedSeq[Long]()\n",
    "\n",
    "  override def getLabel(): IndexedSeq[NDArray] = IndexedSeq()\n",
    "\n",
    "  override def getPad(): Int = 0\n",
    "\n",
    "  override def provideData: ListMap[String, Shape] = _provideData\n",
    "\n",
    "  override def provideLabel: ListMap[String, Shape] = _provideLabel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we provide a function to obtain the data iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mgetDataIter\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getDataIter(batchSize: Int) ={\n",
    "    val (dataTrain, dataTest) = (new BDataIter(\"/Users/roshanin/ml-100k/u1.base\", batchSize), new BDataIter(\"/Users/roshanin/ml-100k/u1.test\", batchSize))\n",
    "    (dataTrain, dataTest)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we calculate the numbers of users and items for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mmaxId\u001b[0m\n",
       "\u001b[36mmu\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m944\u001b[0m\n",
       "\u001b[36mmi\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m1683\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def maxId(fname: String) ={\n",
    "    var mu = 0\n",
    "    var mi =0\n",
    "    for (line <- Source.fromFile(fname).getLines()) {\n",
    "        val arr = line.split(\"\\t\").map(_.trim)\n",
    "        \n",
    "        if(arr.length == 4){\n",
    "            mu = mu max arr(0).toInt  \n",
    "            mi = mi max arr(1).toInt\n",
    "        }\n",
    "    }\n",
    "    (mu+1, mi+1)\n",
    "}\n",
    "\n",
    "val (mu, mi) = maxId(\"/Users/roshanin/ml-100k/u.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "We first implement the RMSE (root-mean-square error) measurement, which is commonly used by matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mRMSE\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def RMSE(label: NDArray, pred: NDArray): Float={\n",
    "    val labelArr = label.toArray\n",
    "    val predArr = pred.toArray\n",
    "\n",
    "    var ret: Float = 0.0f\n",
    "    var n: Float = 0.0f\n",
    "    \n",
    "    for(i <- 0 to labelArr.length-1){\n",
    "        ret += (labelArr(i) - predArr(i)) * (labelArr(i) - predArr(i))\n",
    "        n += 1.0f\n",
    "    }\n",
    "    Math.sqrt(ret/n).asInstanceOf[Float]    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a general training module, which is borrowed from the image classification application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mtrain\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(network: Symbol, batchSize: Int, numEpoch: Int, learningRate: Float) {\n",
    "    var batchSize = 64\n",
    "    val (trainIter, testIter) = getDataIter(batchSize)\n",
    "    val evalMetric = new CustomMetric(RMSE, name = \"rmse\")\n",
    "    \n",
    "//     val model = FeedForward.newBuilder(network)\n",
    "//       .setContext(Context.cpu(0))\n",
    "//       .setNumEpoch(numEpoch)\n",
    "//       .setOptimizer(new SGD(learningRate = learningRate, momentum = 0.9f, wd = 0.0001f))\n",
    "//       .setTrainData(trainIter)\n",
    "//       .setEvalMetric(evalMetric)\n",
    "//       .setEvalData(testIter)\n",
    "//       .setBatchEndCallback(new Speedometer(batchSize, 20000/batchSize))\n",
    "//       .build()\n",
    "    \n",
    "    val model = new FeedForward(ctx = Context.gpu(0),\n",
    "                                symbol = network,\n",
    "                                numEpoch = numEpoch,\n",
    "                                optimizer = new SGD(learningRate = learningRate, momentum = 0.9f, wd = 0.0001f))\n",
    "    \n",
    "\n",
    "      model.fit(trainData = trainIter,\n",
    "                evalData = testIter,\n",
    "                evalMetric = evalMetric,\n",
    "                kvStore = null,\n",
    "                batchEndCallback = new Speedometer(batchSize, 20000/batchSize),\n",
    "               epochEndCallback = null) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks\n",
    "Now we try various networks. We first learn the latent vectors directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (MXNetJVM).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mplainNet\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plainNet(k: Int) ={\n",
    "    // input\n",
    "    val user = Symbol.Variable(\"user\")\n",
    "    val item = Symbol.Variable(\"item\")\n",
    "    val score = Symbol.Variable(\"score\")\n",
    "    // user feature lookup\n",
    "    val user1 = Symbol.Embedding()()(Map(\"data\" -> user, \"input_dim\" -> mu,\n",
    "                                        \"output_dim\" -> k))\n",
    "    // item feature lookup\n",
    "    val item1 = Symbol.Embedding()()(Map(\"data\" -> item, \"input_dim\" -> mi,\n",
    "                                        \"output_dim\" -> k))\n",
    " \n",
    "    // predict by the inner product, which is elementwise product and then sum\n",
    "    \n",
    "    val pred0 = user1 * item1\n",
    "    \n",
    "    val pred1 = Symbol.sum_axis()()(Map(\"data\" -> pred0, \"axis\" -> 1))\n",
    "    val pred2 = Symbol.Flatten()()(Map(\"data\" -> pred1))\n",
    "    // loss layer\n",
    "    val pred = Symbol.LinearRegressionOutput()()(Map(\"data\" -> pred2, \"label\" -> score))\n",
    "    \n",
    "    pred\n",
    "}\n",
    "\n",
    "train(plainNet(64), batchSize=64, numEpoch=10, learningRate=.05f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try to use 2 layers neural network to learn the latent variables, which stack a fully connected layer above the embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mgetOneLayerMlp\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getOneLayerMlp(hidden: Int, k: Int) ={\n",
    "    // input\n",
    "    val user = Symbol.Variable(\"user\")\n",
    "    val item = Symbol.Variable(\"item\")\n",
    "    val score = Symbol.Variable(\"score\")\n",
    "    // user feature lookup\n",
    "    val user1 = Symbol.Embedding()()(Map(\"data\" -> user, \"input_dim\" -> mu,\n",
    "                                        \"output_dim\" -> k))\n",
    "    val user2 = Symbol.Activation()()(Map(\"data\" -> user1, \"act_type\" -> \"relu\"))\n",
    "    val user3 = Symbol.FullyConnected()()(Map(\"data\" -> user2, \"num_hidden\" -> hidden))\n",
    "                          \n",
    "    // item feature lookup\n",
    "    val item1 = Symbol.Embedding()()(Map(\"data\" -> item, \"input_dim\" -> mi,\n",
    "                                        \"output_dim\" -> k))\n",
    "    val item2 = Symbol.Activation()()(Map(\"data\" -> item1, \"act_type\" -> \"relu\"))\n",
    "    val item3 = Symbol.FullyConnected()()(Map(\"data\" -> item2, \"num_hidden\" -> hidden))\n",
    " \n",
    "    // predict by the inner product\n",
    "    \n",
    "    val pred0 = user3 * item3\n",
    "    \n",
    "    val pred1 = Symbol.sum_axis()()(Map(\"data\" -> pred0, \"axis\" -> 1))\n",
    "    val pred2 = Symbol.Flatten()()(Map(\"data\" -> pred1))\n",
    "                          \n",
    "    // loss layer\n",
    "    val pred = Symbol.LinearRegressionOutput()()(Map(\"data\" -> pred2, \"label\" -> score))\n",
    "    pred\n",
    "    \n",
    "}\n",
    "\n",
    "train(getOneLayerMlp(64,64), batchSize=64, numEpoch=10, learningRate=.005f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dropout layers to relief the over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mgetOneLayerDropoutMlp\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getOneLayerDropoutMlp(hidden: Int, k: Int) ={\n",
    "    // input\n",
    "    val user = Symbol.Variable(\"user\")\n",
    "    val item = Symbol.Variable(\"item\")\n",
    "    val score = Symbol.Variable(\"score\")\n",
    "    // user feature lookup\n",
    "    val user1 = Symbol.Embedding(\"user\")()(Map(\"data\" -> user, \"input_dim\" -> mu,\n",
    "                                        \"output_dim\" -> k))\n",
    "    val user2 = Symbol.Activation()()(Map(\"data\" -> user1, \"act_type\" -> \"relu\"))\n",
    "    val user3 = Symbol.FullyConnected()()(Map(\"data\" -> user2, \"num_hidden\" -> hidden))\n",
    "    val user4 = Symbol.Dropout()()(Map(\"data\" -> user3, \"p\" -> 0.5f))\n",
    "                          \n",
    "    // item feature lookup\n",
    "    val item1 = Symbol.Embedding(\"item\")()(Map(\"data\" -> item, \"input_dim\" -> mi,\n",
    "                                        \"output_dim\" -> k))\n",
    "    val item2 = Symbol.Activation()()(Map(\"data\" -> item1, \"act_type\" -> \"relu\"))\n",
    "    val item3 = Symbol.FullyConnected()()(Map(\"data\" -> item2, \"num_hidden\" -> hidden))\n",
    "    val item4 = Symbol.Dropout()()(Map(\"data\" -> item3, \"p\" -> 0.5f))\n",
    "\n",
    "    // predict by the inner product\n",
    "    \n",
    "    val pred0 = user4 * item4\n",
    "    \n",
    "    val pred1 = Symbol.sum_axis()()(Map(\"data\" -> pred0, \"axis\" -> 1))\n",
    "    val pred2 = Symbol.Flatten()()(Map(\"data\" -> pred1))\n",
    "                          \n",
    "    // loss layer\n",
    "    val pred = Symbol.LinearRegressionOutput()()(Map(\"data\" -> pred2, \"label\" -> score))\n",
    "    pred\n",
    "    \n",
    "}\n",
    "                          \n",
    "train(getOneLayerDropoutMlp(256, 512), batchSize=64, numEpoch=10, learningRate=.005f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
