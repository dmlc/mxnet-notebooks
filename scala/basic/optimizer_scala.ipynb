{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "In gradient-base optimization algorithms, we update the parameters (or weights) using the gradients in each iteration. We call this updating function as Optimizer.\n",
    "\n",
    "The main method of an optimizer is update(weight, grad), which updates a NDArray weight using a NDArray gradient. But given that a multi-layer neural network often has more than one weights, we assign each weight a unique integer index. Furthermore, an optimizer may need space to store auxiliary state, such as momentum, we also allow a user-defined state for updating. In summary, an optimizer has two major methods\n",
    "\n",
    "- createState(index, weight): create auxiliary state for the index-th weight.\n",
    "- update(index, weight, grad, state): update the index-th weight given the gradient and auxiliary state. The state can be also updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Scala kernel\n",
    "Add mxnet scala jar which is created as a part of MXNet Scala package installation in classpath as follows:\n",
    "\n",
    "**Note**: Process to add this jar in your scala kernel classpath can differ according to the scala kernel you are using.\n",
    "\n",
    "We have used [jupyter-scala kernel](https://github.com/alexarchambault/jupyter-scala) for creating this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "classpath.addPath(<path_to_jar>)\n",
    "\n",
    "e.g\n",
    "classpath.addPath(\"mxnet-full_2.11-osx-x86_64-cpu-0.1.2-SNAPSHOT.jar\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "### Create and Update\n",
    "MXNet has already implemented several popular optimizers in [optimizer.scala](https://github.com/dmlc/mxnet/blob/master/scala-package/core/src/main/scala/ml/dmlc/mxnet/Optimizer.scala). An convenient way to create one is by using new SGD(args). The following codes create a standard SGD updater which does\n",
    "\n",
    "```scala\n",
    "weight = weight - learning_rate * grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the optimizer you want to use as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.optimizer.SGD\u001b[0m\n",
       "\u001b[36mopt\u001b[0m: \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32moptimizer\u001b[0m.\u001b[32mSGD\u001b[0m = ml.dmlc.mxnet.optimizer.SGD@4b5a5245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ml.dmlc.mxnet._\n",
    "import ml.dmlc.mxnet.optimizer.SGD\n",
    "val opt = new SGD(learningRate=0.1f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the update function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (MXNetJVM).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgrad\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@a5d8d68a\n",
       "\u001b[36mweight\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@ffb131d2\n",
       "\u001b[36mindex\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m0\u001b[0m\n",
       "\u001b[36mres2_4\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val grad = NDArray.ones(2,3)\n",
    "val weight = NDArray.ones(2,3)\n",
    "val index = 0\n",
    "opt.update(index, weight, grad, NDArray.empty(2,3))\n",
    "weight.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When momentum is non-zero, the sgd optimizer needs extra state. State is of type AnyRef. So, we cast the type to NDArray and then print the value of state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmomOpt\u001b[0m: \u001b[32mSGD\u001b[0m = ml.dmlc.mxnet.optimizer.SGD@143d6181\n",
       "\u001b[36mindex\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m0\u001b[0m\n",
       "\u001b[36mgrad\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@e409d01c\n",
       "\u001b[36mweight\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@ac5011ac\n",
       "\u001b[36mstate\u001b[0m: \u001b[32mAnyRef\u001b[0m = ml.dmlc.mxnet.NDArray@fbcb7abb\n",
       "\u001b[36mres3_6\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m-0.10001F\u001b[0m, \u001b[32m-0.10001F\u001b[0m, \u001b[32m-0.10001F\u001b[0m, \u001b[32m-0.10001F\u001b[0m, \u001b[32m-0.10001F\u001b[0m, \u001b[32m-0.10001F\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val momOpt = new SGD(learningRate = 0.1f, momentum = 0.01f)\n",
    "val index = 0\n",
    "val grad = NDArray.ones(2,3)\n",
    "val weight = NDArray.ones(2,3)\n",
    "val state = momOpt.createState(index, weight)\n",
    "opt.update(index, weight, grad, state)\n",
    "state.asInstanceOf[NDArray].toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Types of Optimizers supported\n",
    "- [AdaDelta](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.AdaDelta)\n",
    "- [AdaGrad](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.AdaGrad)\n",
    "- [Adam](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.Adam)\n",
    "- [SGD](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.SGD)\n",
    "- [SGLD](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.SGLD)\n",
    "- [DCASGD](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.DCASGD)\n",
    "- [NAG](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.NAG)\n",
    "- [RMSProp](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.RMSProp)\n",
    "\n",
    "You can set these optimizers while building a FeedForward network in `.setOptimizer(new SGD(...))` method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "[Optimizer](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.optimizer.AdaGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
