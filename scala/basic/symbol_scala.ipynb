{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Tutorial\n",
    "Besides the tensor computation interface NDArray, another main object in MXNet is the Symbol provided by MXNet.Symbol. A symbol represents a multi-output symbolic expression. They are composited by operators, such as simple matrix operations (e.g. “+”), or a neural network layer (e.g. convolution layer). An operator can take several input variables, produce more than one output variables, and have internal state variables. A variable can be either free, which we can bind with value later, or an output of another symbol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Scala kernel\n",
    "Add mxnet scala jar which is created as a part of MXNet Scala package installation in classpath as follows:\n",
    "\n",
    "**Note**: Process to add this jar in your scala kernel classpath can differ according to the scala kernel you are using.\n",
    "\n",
    "We have used [jupyter-scala kernel](https://github.com/alexarchambault/jupyter-scala) for creating this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "classpath.addPath(<path_to_jar>)\n",
    "\n",
    "e.g\n",
    "classpath.addPath(\"mxnet-full_2.11-osx-x86_64-cpu-0.1.2-SNAPSHOT.jar\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol Composition\n",
    "### Basic Operators\n",
    "The following example composites a simple expression a+b. We first create the placeholders a and b with names using Symbol.Variable, and then construct the desired symbol by using the operator +. When the string name is not given during creating, MXNet will automatically generate a unique name for the symbol, which is the case for c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (MXNetJVM).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.Visualization\u001b[0m\n",
       "\u001b[36ma\u001b[0m: \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@235942ea\n",
       "\u001b[36mb\u001b[0m: \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@5bf2b0e\n",
       "\u001b[36mc\u001b[0m: \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@be1263\n",
       "\u001b[36mres1_5\u001b[0m: (\u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32mSymbol\u001b[0m, \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32mSymbol\u001b[0m, \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32mSymbol\u001b[0m) = \u001b[33m\u001b[0m(\n",
       "  ml.dmlc.mxnet.Symbol@235942ea,\n",
       "  ml.dmlc.mxnet.Symbol@5bf2b0e,\n",
       "  ml.dmlc.mxnet.Symbol@be1263\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ml.dmlc.mxnet._\n",
    "import ml.dmlc.mxnet.Visualization\n",
    "\n",
    "val a = Symbol.Variable(\"a\")\n",
    "val b = Symbol.Variable(\"b\")\n",
    "val c = a + b\n",
    "(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most NDArray operators can be applied to Symbol, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36md\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@3011bf98\n",
       "\u001b[36me\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@4b4a02e8\n",
       "\u001b[36mf\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@7a427309\n",
       "\u001b[36mg\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@1ebdbd6e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// elemental wise times\n",
    "val d = a * b  \n",
    "// matrix multiplication\n",
    "val e = Symbol.dot()(a, b)()\n",
    "// reshape\n",
    "val f = Symbol.Reshape()(d+e)()  \n",
    "// broadcast\n",
    "val g = Symbol.broadcast_to()(f)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization:\n",
    "\n",
    "MXNet Scala package uses a simplified implementation of the python-Graphviz library functionality based on: https://github.com/xflr6/graphviz/tree/master/graphviz. You can find the detailed [source code here](https://github.com/dmlc/mxnet/blob/master/scala-package/core/src/main/scala/ml/dmlc/mxnet/Visualization.scala).\n",
    "\n",
    "To visualize the network, create a folder to save the images or pdfs and provide its path in `dot.render()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdot\u001b[0m: \u001b[32mVisualization\u001b[0m.\u001b[32mDot\u001b[0m = ml.dmlc.mxnet.Visualization$Dot@7216eb27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dot = Visualization.plotNetwork(symbol = g)\n",
    "dot.render(engine = \"dot\", fileName = \"g\", path = \"visualization_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Networks\n",
    "Besides the basic operators, Symbol has a rich set of neural network layers. The following codes construct a two layer fully connected neural work and then visualize the structure by given the input data shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@17c5fdf5\n",
       "\u001b[36mfc1\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@40d63146\n",
       "\u001b[36mact1\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@260d2ef\n",
       "\u001b[36mfc2\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@619ecc76\n",
       "\u001b[36mnet\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@590cafe5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Output may vary\n",
    "val data = Symbol.Variable(\"data\")\n",
    "val fc1 = Symbol.FullyConnected(name = \"fc1\")()(Map(\"data\" -> data, \"num_hidden\" -> 128))\n",
    "val act1 = Symbol.Activation(name = \"relu1\")()(Map(\"data\" -> fc1, \"act_type\" -> \"relu\"))\n",
    "val fc2 = Symbol.FullyConnected(name = \"fc2\")()(Map(\"data\" -> act1, \"num_hidden\" -> 10))\n",
    "val net = Symbol.SoftmaxOutput(name = \"out\")()(Map(\"data\" -> fc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdot\u001b[0m: \u001b[32mVisualization\u001b[0m.\u001b[32mDot\u001b[0m = ml.dmlc.mxnet.Visualization$Dot@659aaad1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dot = Visualization.plotNetwork(symbol = net)\n",
    "dot.render(engine = \"dot\", fileName = \"net\", path = \"visualization_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulelized Construction for Deep Networks\n",
    "For deep networks, such as the Google Inception, constructing layer by layer is painful given the large number of layers. For these networks, we often modularize the construction. Take the Google Inception as an example, we can first define a factory function to chain the convolution layer, batch normalization layer, and Relu activation layer together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mConvFactory\u001b[0m\n",
       "\u001b[36mprev\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@2a8530e5\n",
       "\u001b[36mconvComp\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@6e02f36c\n",
       "\u001b[36mshape\u001b[0m: \u001b[32mShape\u001b[0m = (128,3,28,28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " // Output may vary\n",
    "def ConvFactory(data: Symbol, numFilter: Int, kernel: (Int, Int), stride: (Int, Int) = (1, 1),\n",
    "      pad: (Int, Int) = (0, 0), name: String = \"\", suffix: String = \"\"): Symbol = {\n",
    "    val conv = Symbol.Convolution(s\"conv_${name}${suffix}\")()(\n",
    "        Map(\"data\" -> data, \"num_filter\" -> numFilter, \"kernel\" -> s\"$kernel\",\n",
    "            \"stride\" -> s\"$stride\", \"pad\" -> s\"$pad\"))\n",
    "      \n",
    "    val bn = Symbol.BatchNorm(s\"bn_${name}${suffix}\")()(Map(\"data\" -> conv))\n",
    "      \n",
    "    val act = Symbol.Activation(s\"relu_${name}${suffix}\")()(\n",
    "        Map(\"data\" -> bn, \"act_type\" -> \"relu\"))\n",
    "    act\n",
    "  }\n",
    "\n",
    "val prev = Symbol.Variable(\"PreviosOutput\")\n",
    "val convComp = ConvFactory(data = prev, numFilter = 64, kernel = (7, 7), stride=(2, 2))\n",
    "val shape = Shape(128, 3, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdot\u001b[0m: \u001b[32mVisualization\u001b[0m.\u001b[32mDot\u001b[0m = ml.dmlc.mxnet.Visualization$Dot@25525ebb"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dot = Visualization.plotNetwork(symbol = convComp, title = \"ConvFactory\", shape = Map(\"PreviosOutput\" -> shape), \n",
    "                                    nodeAttrs = Map(\"shape\" -> \"oval\", \"fixedsize\" -> \"false\"))\n",
    "\n",
    "dot.render(engine = \"dot\", fileName = \"ConvFactory\", path = \"visualization_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function that constructs an Inception module based on ConvFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mInceptionFactoryA\u001b[0m\n",
       "\u001b[36mprev\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@64b6ebb9\n",
       "\u001b[36min3a\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@76c10a05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def InceptionFactoryA(data: Symbol, num1x1: Int, num3x3red: Int, num3x3: Int,\n",
    "      numd3x3red: Int, numd3x3: Int, pool: String, proj: Int, name: String): Symbol = {\n",
    "    // 1x1\n",
    "    val c1x1 = ConvFactory(data = data, numFilter = num1x1,\n",
    "        kernel = (1, 1), name = s\"${name}_1x1\")\n",
    "    // 3x3 reduce + 3x3\n",
    "    val c3x3r = ConvFactory(data = data, numFilter = num3x3red,\n",
    "        kernel = (1, 1), name = s\"${name}_3x3\", suffix = \"_reduce\")\n",
    "    val c3x3 = ConvFactory(data = c3x3r, numFilter = num3x3,\n",
    "        kernel = (3, 3), pad = (1, 1), name = s\"${name}_3x3\")\n",
    "    // double 3x3 reduce + double 3x3\n",
    "    val cd3x3r = ConvFactory(data = data, numFilter = numd3x3red,\n",
    "        kernel = (1, 1), name = s\"${name}_double_3x3\", suffix = \"_reduce\")\n",
    "    var cd3x3 = ConvFactory(data = cd3x3r, numFilter = numd3x3,\n",
    "        kernel = (3, 3), pad = (1, 1), name = s\"${name}_double_3x3_0\")\n",
    "    cd3x3 = ConvFactory(data = cd3x3, numFilter = numd3x3,\n",
    "        kernel = (3, 3), pad = (1, 1), name = s\"${name}_double_3x3_1\")\n",
    "    // pool + proj\n",
    "    val pooling = Symbol.Pooling(s\"${pool}_pool_${name}_pool\")()(\n",
    "        Map(\"data\" -> data, \"kernel\" -> \"(3, 3)\", \"stride\" -> \"(1, 1)\",\n",
    "            \"pad\" -> \"(1, 1)\", \"pool_type\" -> pool))\n",
    "    val cproj = ConvFactory(data = pooling, numFilter = proj,\n",
    "        kernel = (1, 1), name = s\"${name}_proj\")\n",
    "    // concat\n",
    "    val concat = Symbol.Concat(s\"ch_concat_${name}_chconcat\")(c1x1, c3x3, cd3x3, cproj)()\n",
    "    concat\n",
    "  }\n",
    "\n",
    "\n",
    "val prev = Symbol.Variable(\"PreviosOutput\")\n",
    "val in3a = InceptionFactoryA(prev, 64, 64, 64, 64, 96, \"avg\", 32, \"in3a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: node 'bn_in3a_1x1_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_1x1_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_1x1_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_1x1_moving_var', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_reduce_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_reduce_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_reduce_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_reduce_moving_var', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_3x3_moving_var', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_reduce_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_reduce_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_reduce_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_reduce_moving_var', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_0_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_0_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_0_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_0_moving_var', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_1_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_1_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_1_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_double_3x3_1_moving_var', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_proj_gamma', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_proj_beta', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_proj_moving_mean', graph 'plot' size too small for label\n",
      "Warning: node 'bn_in3a_proj_moving_var', graph 'plot' size too small for label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdot\u001b[0m: \u001b[32mVisualization\u001b[0m.\u001b[32mDot\u001b[0m = ml.dmlc.mxnet.Visualization$Dot@185795b5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dot = Visualization.plotNetwork(symbol=in3a, shape = Map(\"PreviosOutput\" -> shape))\n",
    "\n",
    "dot.render(engine = \"dot\", fileName = \"InceptionFactoryA\", path = \"visualization_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can obtain the whole network by chaining multiple inception modulas. A complete example is available at [visualization example](https://github.com/dmlc/mxnet/tree/master/scala-package/examples/src/main/scala/ml/dmlc/mxnet/examples/visualization)\n",
    "### Group Multiple Symbols\n",
    "To construct neural networks with multiple loss layers, we can use mxnet.Symbol.Group to group multiple symbols together. The following example group two outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@4b091529\n",
       "\u001b[36mfc1\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@68a10ba5\n",
       "\u001b[36mnet\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@436e604\n",
       "\u001b[36mout1\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@162e1cb2\n",
       "\u001b[36mout2\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@150de303\n",
       "\u001b[36mgroup\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@75112e24\n",
       "\u001b[36mres10_6\u001b[0m: \u001b[32mIndexedSeq\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArrayBuffer\u001b[0m(\u001b[32m\"softmax_output\"\u001b[0m, \u001b[32m\"regression_output\"\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val data = Symbol.Variable(\"data\")\n",
    "val fc1 = Symbol.FullyConnected(name = \"fc1\")()(Map(\"data\" -> data, \"num_hidden\" -> 128))\n",
    "val net = Symbol.Activation(name = \"relu1\")()(Map(\"data\" -> fc1, \"act_type\" -> \"relu\"))\n",
    "val out1 = Symbol.SoftmaxOutput(name = \"softmax\")()(Map(\"data\" -> act1))\n",
    "val out2 = Symbol.LinearRegressionOutput(\"regression\")()(Map(\"data\" -> net))\n",
    "val group = Symbol.Group(out1,out2)\n",
    "group.listOutputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations to NDArray\n",
    "As can be seen now, both Symbol and NDArray provide multi-dimensional array operations, such as c=a+b in MXNet. Sometimes users are confused which way to use. We briefly clarify the difference here, more detailed explanation are available [here](http://mxnet.io/architecture/program_model.html).\n",
    "\n",
    "The NDArray provides an imperative programming alike interface, in which the computations are evaluated sentence by sentence. While Symbol is closer to declarative programming, in which we first declare the computation, and then evaluate with data. Examples in this category include regular expression and SQL.\n",
    "\n",
    "The pros for NDArray:\n",
    "\n",
    "- straightforward\n",
    "- easy to work with other language features (for loop, if-else condition, ..) and libraries (numpy, ..)\n",
    "- easy to step-by-step debug\n",
    "\n",
    "The pros for Symbol:\n",
    "\n",
    "- provides almost all functionalities of NDArray, such as +, *, sin, and reshape\n",
    "- provides a large number of neural network related operators such as Convolution, Activation, and BatchNorm\n",
    "- provides automatic differentiation\n",
    "- easy to construct and manipulate complex computations such as deep neural networks\n",
    "- easy to save, load, and visualization\n",
    "- easy for the backend to optimize the computation and memory usage\n",
    "\n",
    "We will show on the mixed programming tutorial how these two interfaces can be used together to develop a complete training program. This tutorial will focus on the usage of Symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol Manipulation *\n",
    "One important difference of Symbol comparing to NDArray is that, we first declare the computation, and then bind with data to run.\n",
    "\n",
    "In this section we introduce the functions to manipulate a symbol directly. But note that, most of them are wrapped nicely by the mx.module. One can skip this section safely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Inference\n",
    "For each symbol, we can query its inputs (or arguments) and outputs. We can also inference the output shape by given the input shape, which facilitates memory allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      ""
     ]
    }
   ],
   "source": [
    "val argName = c.listArguments()  // get the names of the inputs\n",
    "val outName = c.listOutputs()    // get the names of the outputs\n",
    "val (argShape, outShape, _) = c.inferShape(Map(\"a\" -> Shape(2,3), \"b\" -> Shape(2,3))\n",
    "println(argName, argShape)\n",
    "println(outName, outShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind with Data and Evaluate\n",
    "The symbol c we constructed declares what computation should be run. To evaluate it, we need to feed arguments, namely free variables, with data first. We can do it by using the bind method, which accepts device context and a dict mapping free variable names to NDArrays as arguments and returns an executor. The executor provides method forward for evaluation and attribute outputs to get all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of outputs = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mex\u001b[0m: \u001b[32mExecutor\u001b[0m = ml.dmlc.mxnet.Executor@1ba98cde\n",
       "\u001b[36mres11_3\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m2.0F\u001b[0m, \u001b[32m2.0F\u001b[0m, \u001b[32m2.0F\u001b[0m, \u001b[32m2.0F\u001b[0m, \u001b[32m2.0F\u001b[0m, \u001b[32m2.0F\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val ex = c.bind(ctx=Context.cpu(), args=Map(\"a\" -> NDArray.ones(2,3), \n",
    "                                \"b\" -> NDArray.ones(2,3)))\n",
    "ex.forward()\n",
    "println(\"number of outputs = \"+ ex.outputs.length)\n",
    "ex.outputs(0).toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the same symbol on GPU with different data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mex_gpu\u001b[0m: \u001b[32mExecutor\u001b[0m = ml.dmlc.mxnet.Executor@796da522\n",
       "\u001b[36mres12_2\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m, \u001b[32m5.0F\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val ex_gpu = c.bind(ctx=Context.cpu(), args=Map(\"a\" -> NDArray.ones(shape=Shape(3,4), Context.cpu(), dtype = DType.Float32)*2,\n",
    "                                    \"b\" -> NDArray.ones(shape=Shape(3,4), Context.cpu(), dtype = DType.Float32)*3))\n",
    "ex_gpu.forward()\n",
    "ex_gpu.outputs(0).toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save\n",
    "Similar to NDArray, we can serialize a Symbol object by using save and load methods directly. Different to the binary format chosen by NDArray, Symbol uses the more readable json format for serialization. The toJson method returns the json string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"a\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"b\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"elemwise_add\", \n",
      "      \"name\": \"_plus0\", \n",
      "      \"inputs\": [[0, 0, 0], [1, 0, 0]]\n",
      "    }\n",
      "  ], \n",
      "  \"arg_nodes\": [0, 1], \n",
      "  \"node_row_ptr\": [0, 1, 2, 3], \n",
      "  \"heads\": [[2, 0, 0]], \n",
      "  \"attrs\": {\"mxnet_version\": [\"int\", 904]}\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mc2\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@6561a992\n",
       "\u001b[36mres13_3\u001b[0m: \u001b[32mBoolean\u001b[0m = \u001b[32mtrue\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(c.toJson)\n",
    "c.save(\"symbol-c.json\")\n",
    "val c2 = Symbol.load(\"symbol-c.json\")\n",
    "c.toJson == c2.toJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Symbol *\n",
    "Most operators such as Symbol.Convolution and Symbol.Reshape are implemented in C++ for better performance. MXNet also allows users to write new operators using any frontend language such as Python/Scala. It often makes the developing and debugging much easier.\n",
    "\n",
    "To implement an operator in Python, we just need to define the two computation methods forward and backward with several methods for querying the properties, such as listArguments and inferShape.\n",
    "\n",
    "NDArray is the default type of arguments in both forward and backward. Therefore we often also implement the computation with  NDArray operations. \n",
    "\n",
    "We first create a subclass of Operator.CustomOp and then define forward and backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mSoftmax\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  class Softmax(_param: Map[String, String]) extends CustomOp {\n",
    "\n",
    "    override def forward(sTrain: Boolean, req: Array[String],\n",
    "      inData: Array[NDArray], outData: Array[NDArray], aux: Array[NDArray]): Unit = {\n",
    "      val xShape = inData(0).shape\n",
    "      val x = inData(0).toArray.grouped(xShape(1)).toArray\n",
    "      val yArr = x.map { it =>\n",
    "        val max = it.max\n",
    "        val tmp = it.map(e => Math.exp(e.toDouble - max).toFloat)\n",
    "        val sum = tmp.sum\n",
    "        tmp.map(_ / sum)\n",
    "      }.flatten\n",
    "      val y = NDArray.empty(xShape, outData(0).context)\n",
    "      y.set(yArr)\n",
    "      this.assign(outData(0), req(0), y)\n",
    "      y.dispose()\n",
    "    }\n",
    "\n",
    "    override def backward(req: Array[String], outGrad: Array[NDArray],\n",
    "      inData: Array[NDArray], outData: Array[NDArray],\n",
    "      inGrad: Array[NDArray], aux: Array[NDArray]): Unit = {\n",
    "      val l = inData(1).toArray.map(_.toInt)\n",
    "      val oShape = outData(0).shape\n",
    "      val yArr = outData(0).toArray.grouped(oShape(1)).toArray\n",
    "      l.indices.foreach { i =>\n",
    "        yArr(i)(l(i)) -= 1.0f\n",
    "      }\n",
    "      val y = NDArray.empty(oShape, inGrad(0).context)\n",
    "      y.set(yArr.flatten)\n",
    "      this.assign(inGrad(0), req(0), y)\n",
    "      y.dispose()\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use CustomOp.assign to assign the results to mxnet.NDArray based on the value of req, which could be \"over write\" or \"add to\".\n",
    "Next we create a subclass of Operator.CustomOpProp for querying the properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mSoftmaxProp\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " class SoftmaxProp(needTopGrad: Boolean = false)\n",
    "    extends CustomOpProp(needTopGrad) {\n",
    "\n",
    "    override def listArguments(): Array[String] = Array(\"data\", \"label\")\n",
    "\n",
    "    override def listOutputs(): Array[String] = Array(\"output\")\n",
    "\n",
    "    override def inferShape(inShape: Array[Shape]):\n",
    "      (Array[Shape], Array[Shape], Array[Shape]) = {\n",
    "      val dataShape = inShape(0)\n",
    "      val labelShape = Shape(dataShape(0))\n",
    "      val outputShape = dataShape\n",
    "      (Array(dataShape, labelShape), Array(outputShape), null)\n",
    "    }\n",
    "\n",
    "    override def createOperator(ctx: String, inShapes: Array[Array[Int]],\n",
    "      inDtypes: Array[Int]): CustomOp = new Softmax(this.kwargs)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use Symbol.Custom with the register name to use this operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "val mlp = Symbol.Custom(\"softmax\")()(Map(\"data\" -> fc3,\n",
    "        \"label\" -> label, \"op_type\" -> \"softmax\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usages *\n",
    "### Type Cast\n",
    "MXNet uses 32-bit float in default. Sometimes we want to use a lower precision data type for better accuracy-performance trade-off. For example, The Nvidia Tesla Pascal GPUs (e.g. P100) have improved 16-bit float performance, while GTX Pascal GPUs (e.g. GTX 1080) are fast on 8-bit integers.\n",
    "\n",
    "We can use the Symbol.Cast operator to convert the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ListBuffer(Float32),ListBuffer(Float16))\n",
      "(ListBuffer(Int32),ListBuffer(UInt8))"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36ma\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@36fc98c1\n",
       "\u001b[36mb\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@5f318e23\n",
       "\u001b[36margb\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mDType\u001b[0m.\u001b[32mDType\u001b[0m] = \u001b[33mListBuffer\u001b[0m(Float32)\n",
       "\u001b[36moutb\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mDType\u001b[0m.\u001b[32mDType\u001b[0m] = \u001b[33mListBuffer\u001b[0m(Float16)\n",
       "\u001b[36mc\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@1b08cc90\n",
       "\u001b[36margc\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mDType\u001b[0m.\u001b[32mDType\u001b[0m] = \u001b[33mListBuffer\u001b[0m(Int32)\n",
       "\u001b[36moutc\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mDType\u001b[0m.\u001b[32mDType\u001b[0m] = \u001b[33mListBuffer\u001b[0m(UInt8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val a = Symbol.Variable(\"data\")\n",
    "val b = Symbol.Cast()()(Map(\"data\" -> a, \"dtype\" -> \"float16\"))\n",
    "val (argb, outb, _) = b.inferType(Map(\"data\" -> DType.Float32))\n",
    "println(argb, outb)\n",
    "\n",
    "val c = Symbol.Cast()()(Map(\"data\" -> a, \"dtype\" -> \"uint8\"))\n",
    "val (argc, outc, _) = c.inferType(Map(\"data\" -> DType.Int32))\n",
    "print(argc, outc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Sharing\n",
    "Sometimes we want to share the contents between several symbols. This can be simply done by bind these symbols with the same array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36ma\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@cc2dcb9\n",
       "\u001b[36mb\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@2cafc430\n",
       "\u001b[36mc\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@42dfc2a1\n",
       "\u001b[36md\u001b[0m: \u001b[32mSymbol\u001b[0m = ml.dmlc.mxnet.Symbol@471f7c4b\n",
       "\u001b[36mdata\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@c8e1e3e4\n",
       "\u001b[36mex\u001b[0m: \u001b[32mExecutor\u001b[0m = ml.dmlc.mxnet.Executor@3b2f4224\n",
       "\u001b[36mres17_7\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m6.0F\u001b[0m, \u001b[32m6.0F\u001b[0m, \u001b[32m6.0F\u001b[0m, \u001b[32m6.0F\u001b[0m, \u001b[32m6.0F\u001b[0m, \u001b[32m6.0F\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val a = Symbol.Variable(\"a\")\n",
    "val b = Symbol.Variable(\"b\")\n",
    "val c = Symbol.Variable(\"c\")\n",
    "val d = a + b * c\n",
    "\n",
    "val data = NDArray.ones(2,3)*2\n",
    "val ex = d.bind(ctx=Context.cpu(), args=Map(\"a\" -> data, \"b\" -> data, \"c\" -> data))\n",
    "ex.forward()\n",
    "ex.outputs(0).toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [NDArray API](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.NDArray)\n",
    "- [Symbol API](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.Symbol)\n",
    "- [Visualization API](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.Visualization$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
