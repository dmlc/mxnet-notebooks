{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision in MXNet\n",
    "\n",
    "Welcome to the tutorial! This tutorial will guide you through an example of Computer Vision on the MNIST dataset using MXNet. In the tutorial, we will cover: \n",
    "- What is an artificial neuron? \n",
    "- data loading and the MNIST data set\n",
    "- convolutional layers, relu, and softmax layers \n",
    "- constructing a feed-forward model\n",
    "- debugging \n",
    "\n",
    "First let's import the modules and setup logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will cover our data and network architecture, MNIST and LeNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a data set of handwritten digits with corresponding labels. You can think of each image as a 28x28 matrix, each of which is comprised of a greyscale value, and includes a <b> label </b> denoting the digit it contains. \n",
    "\n",
    "<b>Label:</b> 7 \n",
    "<img src=\"https://thatindiandude.github.io/images/im2mat.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We can load the MNIST data from scikit and shuffle it to generate <b> test </b> and <b> training </b> sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAA9CAYAAABbalkHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe50d7d0828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "import tempfile\n",
    "test_data_home = tempfile.mkdtemp()\n",
    "\n",
    "mnist = fetch_mldata('MNIST original', data_home=test_data_home)\n",
    "\n",
    "# shuffle data\n",
    "X, y = shuffle(mnist.data, mnist.target)\n",
    "\n",
    "# split dataset\n",
    "train_data = X[:50000, :].astype('float32')\n",
    "train_data = train_data.reshape((len(train_data), 1, 28, 28))\n",
    "train_label = y[:50000]\n",
    "\n",
    "val_data = X[50000: 60000, :].astype('float32')\n",
    "val_data = val_data.reshape((len(val_data), 1, 28, 28))\n",
    "val_label = y[50000:60000]\n",
    "\n",
    "# Normalize data\n",
    "train_data[:] /= 256.0\n",
    "val_data[:] /= 256.0\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.imshow(X[i].reshape((28,28)), cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "\n",
    "The LeNet is an early hallmark architecture on the MNIST data.\n",
    "<img src=\"https://thatindiandude.github.io/images/lenet.png\">\n",
    "\n",
    "LeCun, Yann, Leon Bottou, Yoshua Bengio, and Patrick Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE (1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "data = mx.symbol.Variable('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "### Receptive Field, Kernel, and Stride\n",
    "When we specify the <b>kernel</b> size, we are defining a “<b>receptive field</b>” or region of the image which we will use in an operation. One can also specify a stride to space out the receptive fields, although MXNet will use a (1,1) stride by default. \n",
    "<img src=\"https://thatindiandude.github.io/images/receptive_field.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let k be the filter id. For each receptive field, we have a randomly initialized weight matrix such that, when multiplied by the receptive field, we have a real-valued constant result: $$I_{ij} * W_{ijk} = c \\in \\mathbb{R} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $k$=1, then we will have the following feature map: \n",
    "<img src=\"https://thatindiandude.github.io/images/convolution.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $k>1$, we apply multiple randomly initialized weights to the same receptive fields to result in several feature maps:\n",
    "<img src=\"https://thatindiandude.github.io/images/multi_filter.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By keeping the weight matrices independent between filters, we hope that each one captures different features from the input image. We will “stack these” (indicated by the stacked rectangles in the LeNet architecture image) to build a 3-dimensional <b>tensor</b>:\n",
    "<img src=\"https://thatindiandude.github.io/images/conv_output.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first conv layer\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "We then take the resulting tensor and apply an element-wise “activation” function. Using the $tanh$ function, we introduce non-linearity to the feature map.  \n",
    "<img src=\"https://thatindiandude.github.io/images/activation_map.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling \n",
    "\n",
    "<img src=\"https://thatindiandude.github.io/images/pooling.png\" style=\"height: 75%; width: 75%;\">\n",
    "Picture from Stanford's CS231n notes: http://cs231n.github.io/convolutional-networks/#fc\n",
    "\n",
    "We will take another kernel (with stride) and take the max value of that kernel from the resulting activation map. Then, we stitch these together to create the pooling layer. In doing so, we downsample the convolutional output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                          kernel=(2,2), stride=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "When we apply max-pooling, we want to find a feature in the activation map and to get rid of information regarding its locality. We don’t need this information because what matters is how the feature exists, relative to other locations of appearance close-by in the activation map rather than relative to the whole image. We can also reduce overhead in terms of the number of parameters needed to process the output from the convolution operation. \n",
    " \n",
    "When we apply the tanh function and pooling twice, the results may detect edges, corners, and dots in a first layer, and higher level features in the second layer due to more non-linearity. \n",
    "\n",
    "Jarrett et al. use tanh and absolute value pooling to detect edges  \n",
    "\n",
    "<img src=\"https://thatindiandude.github.io/images/jarrett.png\" style=\"height: 75%; width: 75%;\">\n",
    "Jarrett, Kevin, Koray Kavukcuoglu, and Yann Lecun. \"What is the best multi-stage architecture for object recognition?.\" 2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layers\n",
    "As opposed to the convolutional layer, in which receptive fields are only connected to spatially local ones, a fully connected layer will connect all receptive fields from the last layer to construct its activation maps. \n",
    "\n",
    "<img src=\"https://thatindiandude.github.io/images/fc_layer.jpeg\" style=\"height: 50%; width: 50%;\">\n",
    "Picture from Stanford's CS231n notes: http://cs231n.github.io/convolutional-networks/#fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first fully connected layer\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By flattening the pooling later, we can create a flat representation of an embedding of an input image. Then, by using a fully connected layer, we will release the constraint on spatial locality and enable features to mix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by using a 10-unit FC layer, we will have a neuron which is accurate in predicting of one each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# second fully connected layer\n",
    "fc2 = mx.symbol.FullyConnected(data=tanh3, num_hidden=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Prediction\n",
    "Now that we have 10 neurons from which to make predictions, we will use softmax prediction to actually find the most likely digit in an input $x$.\n",
    "\n",
    "#### Process\n",
    "1. Determine case for input being a digit pixel by pixel \n",
    "2. Establish probabilities per digit\n",
    "\n",
    "##### Determine Case\n",
    "For each digit $i$ (0...9), the case is calculated for each pixel $j$ using weight $W_{j}$. We also have a bias term $b$ added per digit. Then, we want to exponentiate this. \n",
    "\n",
    "$$case_i = \\exp{(\\sum_{j}W_{i,j}x_j + b_i})$$ \n",
    "\n",
    "##### Establish Probability\n",
    "\n",
    "Then, we will normalize the case across all digits to establish probabilities. Overall all classes $j$: \n",
    "\n",
    "$$softmax(x) = [ \\frac{\\exp{x_0}}{\\sum_{j}\\exp{x_j}}, \\frac{\\exp{x_1}}{\\sum_{j}\\exp{x_j}}, ... ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "lenet = mx.symbol.SoftmaxOutput(data=fc2, name='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Model\n",
    "Now that we have created a model, we can now run a model. First, we will need an iterator to load our data at an appropriate batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a numpy iterator\n",
    "batch_size = 100\n",
    "train_iter = mx.io.NDArrayIter(train_data, train_label, batch_size=batch_size, shuffle=True)\n",
    "val_iter = mx.io.NDArrayIter(val_data, val_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run the model with several input parameters, including (but not limited to):\n",
    "- context: where to run the model, either CPU(s) or GPU(s)\n",
    "- symbol: model \n",
    "- learning_rate: how fast are weights adjusted\n",
    "- momentum: how much to weigh the rate of change in the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training with [cpu(0)]\n",
      "INFO:root:Epoch[0] Batch [200]\tSpeed: 317.73 samples/sec\tTrain-accuracy=0.361900\n",
      "INFO:root:Epoch[0] Batch [400]\tSpeed: 306.71 samples/sec\tTrain-accuracy=0.941200\n",
      "INFO:root:Epoch[0] Resetting Data Iterator\n",
      "INFO:root:Epoch[0] Time cost=160.885\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.965800\n",
      "INFO:root:Epoch[1] Batch [200]\tSpeed: 309.89 samples/sec\tTrain-accuracy=0.971700\n",
      "INFO:root:Epoch[1] Batch [400]\tSpeed: 308.80 samples/sec\tTrain-accuracy=0.977950\n",
      "INFO:root:Epoch[1] Resetting Data Iterator\n",
      "INFO:root:Epoch[1] Time cost=162.047\n",
      "INFO:root:Epoch[1] Validation-accuracy=0.977700\n",
      "INFO:root:Epoch[2] Batch [200]\tSpeed: 306.15 samples/sec\tTrain-accuracy=0.981400\n",
      "INFO:root:Epoch[2] Batch [400]\tSpeed: 307.56 samples/sec\tTrain-accuracy=0.987000\n",
      "INFO:root:Epoch[2] Resetting Data Iterator\n",
      "INFO:root:Epoch[2] Time cost=163.130\n",
      "INFO:root:Epoch[2] Validation-accuracy=0.979500\n"
     ]
    }
   ],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = mx.cpu(), \n",
    "    symbol = lenet,\n",
    "    num_epoch = 3,        # Train for 3 epochs\n",
    "    learning_rate = 0.1,  # Learning rate\n",
    "    momentum = 0.9,       # Momentum for SGD with momentum\n",
    "    wd = 0.00001 \n",
    ")\n",
    "\n",
    "model.fit(X = train_iter, \n",
    "          eval_data = val_iter, \n",
    "          batch_end_callback = mx.callback.Speedometer(batch_size, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test and Debug\n",
    "We can print out the following test run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPlGA/h9WJWBg6bH0hwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4bc136f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.imshow((X[0].reshape((28,28))*255).astype(np.uint8), cmap='Greys_r')\n",
    "plt.show()\n",
    "model.predict((X[0].reshape(1, 1, 28, 28)))[0].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the model's accuracy on the entire test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.950000000000003"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(val_iter)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
